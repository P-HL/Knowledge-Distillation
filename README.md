# Knowledge-Distillation
About A paper list of  Knowledge-Distillation(processing).

**Update log**
* 12.3 update [From zhang]

## Table of Contents
* [Datasets](https://github.com/PHL22/Knowledge-Distillation/blob/main/README.md#datasets)
* [Paper list](https://github.com/PHL22/Knowledge-Distillation#paper-list)

## Datasets


## Performance tables
|  表头  |  表头  |
| :----:  |  :----:  |
|  单元格  |  单元格  |
|  单元格  |  单元格  |

## Paper list
**From zhang**
* (arXiv 2019.06) Distilling Object Detectors with Fine-grained Feature Imitation. [[paper]](https://arxiv.org/abs/1906.03609v1) [[code]](https://github.com/twangnh/Distilling-Object-Detectors)
* (arXiv 2020.05) Squeezed Deep 6DoF Object Detection Using Knowledge Distillation  --v3. [[paper]](https://arxiv.org/abs/2003.13586) [[code]](https://github.com/heitorcfelix/singleshot6Dpose)
* (arXiv 2020.06) Structured Knowledge Distillation for Dense Prediction. [[paper]](https://arxiv.org/abs/1903.04197v4) [[code]](https://git.io/StructKD)
* (arXiv 2021.02) Localization Distillation for Object Detection. [[paper]](https://arxiv.org/abs/2102.12252v2) [[code]](https://github.com/HikariTJU/LD)
* (arXiv 2021.03) Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection. [[paper]](https://arxiv.org/abs/2103.17115) [[code]](https://github.com/hzhupku/DCNet)
* (arXiv 2021.03) Distilling Object Detectors via Decoupled Features. [[paper]](https://arxiv.org/abs/2103.14475v1) [[code]](https://github.com/ggjy/DeFeat.pytorch)
* (arXiv 2021.03) Robust and Accurate Object Detection via Adversarial Learning  --v2. [[paper]](https://arxiv.org/abs/2103.13886) [[model]](https://arxiv.org/abs/2103.13886)
* (arXiv 2021.03) There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge. [[paper]](https://arxiv.org/abs/2103.01353v1) [[code]](https://github.com/robot-learning-freiburg/MM-DistillNet)
* (arXiv 2021.03) General Instance Distillation for Object Detection --v1. [[paper]](https://arxiv.org/abs/2103.02340v1) 
* (arXiv 2021.04) General Instance Distillation for Object Detection --v2. [[paper]](https://arxiv.org/abs/2103.02340v2)
* (arXiv 2021.04) Distilling Knowledge via Knowledge Review. [[paper]](https://arxiv.org/abs/2104.09044) [[code]](https://github.com/dvlab-research/ReviewKD)
* (arXiv 2021.08) Channel-wise Knowledge Distillation for Dense Prediction. [[paper]](https://arxiv.org/abs/2011.13256) [[code]](https://github.com/irfanICMLL/TorchDistiller/tree/main/SemSeg-distill)
* (arXiv 2021.09) Deep Structured Instance Graph for Distilling Object Detectors. [[paper]](https://arxiv.org/abs/2109.12862) [[code]](https://github.com/dvlab-research/Dsig)
* (arXiv 2021.10) Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection.  [[paper]](https://arxiv.org/abs/2010.12023)
* (arXiv 2021.10) Improving Object Detection by Label Assignment Distillation  --v3. [[paper]](https://arxiv.org/abs/2108.10520v3) [[code]](https://github.com/cybercore-co-ltd/CoLAD)  

* (cvpr 2021) Multi-Scale Aligned Distillation for Low-Resolution Detection. [[paper]](https://jiaya.me/papers/ms_align_distill_cvpr21.pdf) [[code]](https://github.com/Jia-Research-Lab/MSAD)  








**From pan**    




# Contact & Feedback
If you have any suggestions about this project, feel free to contact me.
* [e-mail: hlp@hbut.edu.cn]


